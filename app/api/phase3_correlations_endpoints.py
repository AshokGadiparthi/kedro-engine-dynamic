"""
Phase 3: Advanced Correlations API Endpoints
FastAPI router for advanced correlation analysis
FINAL CUSTOMIZED VERSION - Works with YOUR Dataset model!
"""

from fastapi import APIRouter, HTTPException, Query, Depends
from typing import Optional
import logging
from datetime import datetime
import pandas as pd
import os

# ‚úÖ Correct imports
from app.core.phase3_advanced_correlations import AdvancedCorrelationAnalysis
from app.core.database import get_db
from app.models.models import Dataset
from sqlalchemy.orm import Session

router = APIRouter(prefix="", tags=["Phase 3 - Correlations"])
logger = logging.getLogger(__name__)

from pathlib import Path
import os

import numpy as np
from typing import Any
import pandas as pd

def convert_numpy_types(obj: Any) -> Any:
    """
    Convert ALL numpy types to Python native types for JSON serialization
    ‚úÖ More comprehensive - catches more numpy types
    """
    # Handle numpy generic types
    if isinstance(obj, np.generic):
        # This catches ALL numpy scalar types
        if isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, (np.integer, np.int8, np.int16, np.int32, np.int64,
                              np.uint8, np.uint16, np.uint32, np.uint64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float16, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, np.complexfloating):
            return complex(obj)
        else:
            return obj

    # Handle pandas DataFrames
    elif isinstance(obj, pd.DataFrame):
        return obj.applymap(convert_numpy_types).to_dict('records')

    # Handle pandas Series
    elif isinstance(obj, pd.Series):
        return obj.apply(convert_numpy_types).to_dict()

    # Handle numpy arrays
    elif isinstance(obj, np.ndarray):
        return convert_numpy_types(obj.tolist())

    # Handle dictionaries (recursive)
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}

    # Handle lists and tuples (recursive)
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]

    # Return as-is for other types
    else:
        return obj

KEDRO_PROJECT_PATH = Path("/home/ashok/work/latest/full/kedro-ml-engine-integrated")


def get_dataset_from_db(dataset_id: str, db: Session) -> Optional[pd.DataFrame]:
    """
    Get dataset from database - FIXED VERSION

    ‚úÖ FIXED: Corrected file path retrieval logic

    Your Dataset model has these attributes:
    - id: Dataset ID
    - project_id: Project reference
    - file_name: Original filename
    - kedro_path: ‚úÖ CORRECT PATH (data/01_raw/{project_id}/{filename})
    - created_at: Creation timestamp
    - description: Description text
    - name: Dataset name
    - file_size_bytes: File size

    Args:
        dataset_id: The dataset ID
        db: Database session (dependency injection)

    Returns:
        pandas DataFrame or None if not found
    """
    try:
        logger.info(f"üìÇ Retrieving dataset: {dataset_id}")

        # Query the database using the session
        dataset_record = db.query(Dataset).filter(
            Dataset.id == dataset_id
        ).first()

        if not dataset_record:
            logger.warning(f"‚ö†Ô∏è Dataset not found: {dataset_id}")
            return None

        # ‚úÖ FIX: Get the correct path from your Dataset model
        # Your database has: kedro_path = "data/01_raw/{project_id}/{filename}"

        file_path = None

        # Try to get kedro_path (your actual path column)
        if hasattr(dataset_record, 'kedro_path') and dataset_record.kedro_path:
            file_path = dataset_record.kedro_path
            logger.info(f"‚úÖ Using kedro_path: {file_path}")

        # Fallback: Try other common path attributes
        elif hasattr(dataset_record, 'file_path') and dataset_record.file_path:
            file_path = dataset_record.file_path
            logger.info(f"‚úÖ Using file_path: {file_path}")

        elif hasattr(dataset_record, 'path') and dataset_record.path:
            file_path = dataset_record.path
            logger.info(f"‚úÖ Using path: {file_path}")

        # If still no path, log available attributes
        if not file_path:
            available_attrs = [k for k in dataset_record.__dict__.keys() if not k.startswith('_')]
            logger.error(f"‚ùå No file path found in Dataset attributes")
            logger.error(f"   Available: {available_attrs}")
            logger.error(f"   Expected one of: kedro_path, file_path, path")
            raise HTTPException(
                status_code=500,
                detail=f"Cannot determine file path from dataset. Available attributes: {available_attrs}"
            )

        # ‚úÖ FIX: Build FULL path AFTER we have the relative path
        full_file_path = os.path.join(str(KEDRO_PROJECT_PATH), file_path)
        logger.info(f"üìç Full file path: {full_file_path}")

        # Verify file exists
        if not os.path.exists(full_file_path):
            logger.error(f"‚ùå File not found at path: {full_file_path}")
            logger.error(f"   Relative path: {file_path}")
            logger.error(f"   KEDRO_PROJECT_PATH: {KEDRO_PROJECT_PATH}")
            raise HTTPException(status_code=500, detail=f"Dataset file not found: {full_file_path}")

        # Load the CSV file
        logger.info(f"üìñ Loading CSV from: {full_file_path}")
        df = pd.read_csv(full_file_path)
        logger.info(f"‚úÖ Loaded dataset {dataset_id} with shape {df.shape}")
        return df

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error retrieving dataset {dataset_id}: {str(e)}")
        logger.error(f"   Exception type: {type(e).__name__}")
        logger.error(f"   Exception details: {str(e)}", exc_info=True)
        return None


@router.get("/{dataset_id}/phase3/correlations/enhanced")
async def get_enhanced_correlations(
        dataset_id: str,
        threshold: float = Query(0.3, ge=0.0, le=1.0),
        db: Session = Depends(get_db)
) -> dict:
    """
    Get enhanced correlation analysis

    Parameters:
    - dataset_id: Dataset ID
    - threshold: Correlation threshold (0.0-1.0, default: 0.3)

    Returns:
    - Correlation matrix
    - Correlation pairs above threshold
    - High correlation pairs (>0.7)
    - Very high correlation pairs (>0.9)
    - Multicollinearity pairs
    - Strength distribution
    - Statistics

    Response time: ~200-300ms
    """
    try:
        logger.info(f"üìä Enhanced correlations requested for dataset: {dataset_id}")

        # Get dataset
        df = get_dataset_from_db(dataset_id, db)

        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        # Perform analysis
        analyzer = AdvancedCorrelationAnalysis(df)
        results = analyzer.get_enhanced_correlations(threshold=threshold)

        logger.info(f"‚úÖ Enhanced correlations analysis completed")

        return convert_numpy_types({
            "dataset_id": dataset_id,
            "timestamp": datetime.now().isoformat(),
            "analysis": results,
            "threshold": threshold,
            "analysis_type": "Enhanced Correlations",
            "total_features": len(df.select_dtypes(include=['number']).columns)
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error in enhanced correlations: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error analyzing correlations: {str(e)}")


@router.get("/{dataset_id}/phase3/correlations/vif")
async def get_vif_analysis(
        dataset_id: str,
        db: Session = Depends(get_db)
) -> dict:
    """
    Get Variance Inflation Factor (VIF) analysis

    Returns:
    - VIF scores for each numeric feature
    - Severity levels
    - Problematic features
    - Overall multicollinearity level
    - Interpretation

    VIF Interpretation:
    - VIF < 5: Acceptable
    - VIF 5-10: Moderate multicollinearity (caution)
    - VIF > 10: High multicollinearity (action needed)

    Response time: ~250-350ms
    """
    try:
        logger.info(f"üìà VIF analysis requested for dataset: {dataset_id}")

        df = get_dataset_from_db(dataset_id, db)

        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        analyzer = AdvancedCorrelationAnalysis(df)
        vif_results = analyzer.get_vif_analysis()

        logger.info(f"‚úÖ VIF analysis completed")

        return convert_numpy_types({
            "dataset_id": dataset_id,
            "timestamp": datetime.now().isoformat(),
            "analysis": vif_results,
            "analysis_type": "VIF (Variance Inflation Factor)",
            "interpretation_guide": {
                "low": "VIF < 5: Acceptable multicollinearity",
                "moderate": "VIF 5-10: Moderate multicollinearity - caution recommended",
                "high": "VIF > 10: High multicollinearity - action needed"
            }
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error in VIF analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error in VIF analysis: {str(e)}")


@router.get("/{dataset_id}/phase3/correlations/heatmap-data")
async def get_heatmap_data(
        dataset_id: str,
        db: Session = Depends(get_db)
) -> dict:
    """
    Get correlation heatmap visualization data

    Returns:
    - Heatmap data in list format
    - Column names
    - Min/max correlation values

    Response time: ~150-250ms
    """
    try:
        logger.info(f"üî• Heatmap data requested for dataset: {dataset_id}")

        df = get_dataset_from_db(dataset_id, db)

        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        analyzer = AdvancedCorrelationAnalysis(df)
        heatmap_data = analyzer.get_correlation_heatmap_data()

        logger.info(f"‚úÖ Heatmap data generated")

        return convert_numpy_types({
            "dataset_id": dataset_id,
            "timestamp": datetime.now().isoformat(),
            "heatmap": heatmap_data,
            "analysis_type": "Correlation Heatmap Data"
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error generating heatmap data: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating heatmap: {str(e)}")


@router.get("/{dataset_id}/phase3/correlations/clustering")
async def get_correlation_clustering(
        dataset_id: str,
        db: Session = Depends(get_db)
) -> dict:
    """
    Get feature clustering based on correlations

    Returns:
    - Feature clusters
    - Cluster count
    - Cluster interpretation

    Response time: ~300-400ms
    """
    try:
        logger.info(f"üéØ Correlation clustering requested for dataset: {dataset_id}")

        df = get_dataset_from_db(dataset_id, db)

        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        analyzer = AdvancedCorrelationAnalysis(df)
        clustering = analyzer.get_correlation_clustering()

        logger.info(f"‚úÖ Correlation clustering completed")

        return convert_numpy_types({
            "dataset_id": dataset_id,
            "timestamp": datetime.now().isoformat(),
            "clustering": clustering,
            "analysis_type": "Correlation-Based Feature Clustering"
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error in correlation clustering: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error in clustering: {str(e)}")


@router.get("/{dataset_id}/phase3/correlations/relationship-insights")
async def get_relationship_insights(
        dataset_id: str,
        db: Session = Depends(get_db)
) -> dict:
    """
    Get relationship insights and patterns

    Returns:
    - Strongest positive relationships
    - Strongest negative relationships
    - Uncorrelated pairs
    - Feature connectivity scores
    - Interesting patterns

    Response time: ~200-300ms
    """
    try:
        logger.info(f"üîó Relationship insights requested for dataset: {dataset_id}")

        df = get_dataset_from_db(dataset_id, db)

        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        analyzer = AdvancedCorrelationAnalysis(df)
        insights = analyzer.get_relationship_insights()

        logger.info(f"‚úÖ Relationship insights generated")

        return convert_numpy_types({
            "dataset_id": dataset_id,
            "timestamp": datetime.now().isoformat(),
            "insights": insights,
            "analysis_type": "Relationship Insights"
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error generating relationship insights: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating insights: {str(e)}")


@router.get("/{dataset_id}/phase3/correlations/warnings")
async def get_multicollinearity_warnings(
        dataset_id: str,
        db: Session = Depends(get_db)
) -> dict:
    """
    Get multicollinearity warnings and recommendations

    Returns:
    - Warning list
    - Warning count
    - Overall assessment
    - Specific recommendations

    Response time: ~250-350ms
    """
    try:
        logger.info(f"‚ö†Ô∏è Multicollinearity warnings requested for dataset: {dataset_id}")

        df = get_dataset_from_db(dataset_id, db)

        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        analyzer = AdvancedCorrelationAnalysis(df)
        warnings = analyzer.get_multicollinearity_warnings()

        logger.info(f"‚úÖ Multicollinearity warnings generated")

        return convert_numpy_types({
            "dataset_id": dataset_id,
            "timestamp": datetime.now().isoformat(),
            "warnings": warnings,
            "analysis_type": "Multicollinearity Warnings"
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error generating warnings: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error generating warnings: {str(e)}")


@router.get("/{dataset_id}/phase3/correlations/complete")
async def get_complete_correlation_analysis(
        dataset_id: str,
        threshold: float = Query(0.3, ge=0.0, le=1.0),
        db: Session = Depends(get_db)
) -> dict:
    """
    Get complete correlation analysis (all endpoints combined)

    This is the RECOMMENDED endpoint - use this for best performance!

    Parameters:
    - dataset_id: Dataset ID
    - threshold: Correlation threshold (0.0-1.0, default: 0.3)

    Returns:
    - Enhanced correlations
    - VIF analysis
    - Heatmap data
    - Feature clustering
    - Relationship insights
    - Multicollinearity warnings

    Response time: ~1-2 seconds (combined)
    """
    try:
        logger.info(f"üìä Complete correlation analysis requested for dataset: {dataset_id}")

        df = get_dataset_from_db(dataset_id, db)

        if df is None:
            raise HTTPException(status_code=404, detail=f"Dataset {dataset_id} not found")

        analyzer = AdvancedCorrelationAnalysis(df)

        # Gather all analyses
        complete_analysis = {
            "enhanced_correlations": analyzer.get_enhanced_correlations(threshold=threshold),
            "vif_analysis": analyzer.get_vif_analysis(),
            "heatmap_data": analyzer.get_correlation_heatmap_data(),
            "clustering": analyzer.get_correlation_clustering(),
            "relationship_insights": analyzer.get_relationship_insights(),
            "multicollinearity_warnings": analyzer.get_multicollinearity_warnings()
        }

        logger.info(f"‚úÖ Complete correlation analysis finished")

        return convert_numpy_types({
            "dataset_id": dataset_id,
            "timestamp": datetime.now().isoformat(),
            "analysis": complete_analysis,
            "analysis_type": "Complete Phase 3 Correlation Analysis",
            "components": 6,
            "total_features": len(df.select_dtypes(include=['number']).columns)
        })

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error in complete correlation analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")