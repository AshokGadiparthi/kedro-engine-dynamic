"""
Celery tasks for ML Pipeline execution with Kedro integration
COMPLETE VERSION: Subprocess + All Existing Functionality

Features:
- Non-blocking Kedro execution using subprocess
- Pipeline name validation
- Parameter support
- Comprehensive logging
- Database integration
- Error handling
- All existing tasks (process_data, analyze_data)
- Timeout protection
"""

import os
import subprocess
import logging
from pathlib import Path
from datetime import datetime
from celery_app import app
from app.core.job_manager import JobManager
import sys

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# Initialize DB manager
db_manager = JobManager()

# Get Kedro project path from environment
KEDRO_PROJECT_PATH = Path(os.getenv(
    'KEDRO_PROJECT_PATH',
    '/home/ashok/work/latest/full/kedro-ml-engine-integrated'
))

logger.info(f"‚úÖ Kedro project path configured: {KEDRO_PROJECT_PATH}")

# Valid pipeline names - MUST match your Kedro pipelines!
VALID_PIPELINES = ['__default__', 'data_processing', 'data_loading']


@app.task(name='app.tasks.execute_pipeline', bind=True, time_limit=600)
def execute_pipeline(self, job_id: str, pipeline_name: str, parameters: dict = None):
    """
    Execute a Kedro pipeline using subprocess (NON-BLOCKING!)

    This task:
    1. Validates pipeline name
    2. Runs Kedro as subprocess (doesn't block Celery)
    3. Captures output and logs
    4. Updates database with results
    5. Handles errors gracefully

    Args:
        job_id (str): Unique job identifier
        pipeline_name (str): Name of Kedro pipeline to execute
        parameters (dict): Pipeline parameters (optional)

    Returns:
        dict: Execution result with status and metadata
    """

    job_start_time = datetime.utcnow()
    logger.info(f"{'='*80}")
    logger.info(f"üöÄ STARTING PIPELINE EXECUTION")
    logger.info(f"{'='*80}")
    logger.info(f"Job ID: {job_id}")
    logger.info(f"Pipeline: {pipeline_name}")
    logger.info(f"Parameters: {parameters or {}}")

    try:
        # ====================================================================
        # STEP 1: Update job status
        # ====================================================================
        logger.info(f"\n[STEP 1] Updating job status...")
        db_manager.update_job_status(job_id, "running")
        logger.info(f"‚úÖ Job {job_id} marked as RUNNING")

        # ====================================================================
        # STEP 2: Verify Kedro project exists
        # ====================================================================
        logger.info(f"\n[STEP 2] Verifying Kedro project...")
        if not KEDRO_PROJECT_PATH.exists():
            raise FileNotFoundError(f"Kedro project not found at {KEDRO_PROJECT_PATH}")
        logger.info(f"‚úÖ Kedro project verified: {KEDRO_PROJECT_PATH}")

        # ====================================================================
        # STEP 2.5: Validate pipeline name
        # ====================================================================
        logger.info(f"\n[STEP 2.5] Validating pipeline name...")

        # Check if pipeline name is a UUID (wrong!)
        if '-' in pipeline_name and len(pipeline_name) == 36:
            logger.error(f"‚ùå Invalid pipeline name: '{pipeline_name}'")
            logger.error(f"   Pipeline name appears to be a UUID, not a valid pipeline name!")
            logger.error(f"   Valid pipelines: {VALID_PIPELINES}")
            raise ValueError(
                f"Invalid pipeline name '{pipeline_name}'. "
                f"Valid pipelines are: {', '.join(VALID_PIPELINES)}"
            )

        # Check if pipeline name is valid
        if pipeline_name not in VALID_PIPELINES:
            logger.error(f"‚ùå Pipeline '{pipeline_name}' not found!")
            logger.error(f"   Valid pipelines: {VALID_PIPELINES}")
            raise ValueError(
                f"Pipeline '{pipeline_name}' not found. "
                f"Valid pipelines are: {', '.join(VALID_PIPELINES)}"
            )

        logger.info(f"‚úÖ Pipeline name is valid: {pipeline_name}")

        # ====================================================================
        # STEP 3: Prepare parameters
        # ====================================================================
        logger.info(f"\n[STEP 3] Preparing pipeline parameters...")
        extra_params = parameters or {}

        if extra_params:
            logger.info(f"üìä Using custom parameters:")
            for key, value in extra_params.items():
                logger.info(f"   - {key}: {value}")
        else:
            logger.info(f"‚úÖ Using default parameters")

        # ====================================================================
        # STEP 4: Execute pipeline via subprocess (NON-BLOCKING!)
        # ====================================================================
        logger.info(f"\n[STEP 4] Executing pipeline via subprocess...")

        # Build Kedro command
        python_exe = sys.executable
        cmd = [python_exe, '-m', 'kedro', 'run', '--pipeline', pipeline_name]

        # Add parameters if provided
        if extra_params:
            for key, value in extra_params.items():
                cmd.extend(['--params', f'{key}:{value}'])

        logger.info(f"Command: {' '.join(cmd)}")
        logger.info(f"Working directory: {KEDRO_PROJECT_PATH}")
        logger.info(f"{'='*80}")

        # Run Kedro as subprocess with 5 minute timeout
        try:
            result = subprocess.run(
                cmd,
                cwd=str(KEDRO_PROJECT_PATH),
                capture_output=True,
                text=True,
                timeout=300,  # 5 minute timeout
            )

            # Log output
            if result.stdout:
                logger.info(f"\n[Kedro Output]")
                for line in result.stdout.split('\n'):
                    if line.strip():
                        logger.info(line)

            # Check for errors
            if result.returncode != 0:
                logger.error(f"\n[Kedro Error]")
                if result.stderr:
                    for line in result.stderr.split('\n'):
                        if line.strip():
                            logger.error(line)
                raise RuntimeError(f"Kedro failed with exit code {result.returncode}")

            logger.info(f"\n{'='*80}")
            logger.info(f"‚úÖ Pipeline execution COMPLETED")

        except subprocess.TimeoutExpired:
            logger.error("‚ùå Pipeline execution timed out (>5 minutes)")
            raise TimeoutError("Kedro pipeline execution exceeded 5 minute timeout")
        except Exception as e:
            logger.error(f"‚ùå Pipeline execution failed: {e}")
            raise

        # ====================================================================
        # STEP 5: Prepare result
        # ====================================================================
        logger.info(f"\n[STEP 5] Preparing execution result...")

        execution_time = (datetime.utcnow() - job_start_time).total_seconds()

        result = {
            "status": "completed",
            "pipeline_name": pipeline_name,
            "message": f"Pipeline '{pipeline_name}' executed successfully",
            "execution_time": execution_time,
            "parameters_used": extra_params,
            "timestamp": job_start_time.isoformat()
        }

        logger.info(f"‚úÖ Result prepared:")
        logger.info(f"   - Status: {result['status']}")
        logger.info(f"   - Execution Time: {execution_time:.2f}s")

        # ====================================================================
        # STEP 6: Store results in database
        # ====================================================================
        logger.info(f"\n[STEP 6] Storing results in database...")

        db_manager.update_job_results(job_id, result)
        logger.info(f"‚úÖ Results stored for job {job_id}")

        # ====================================================================
        # SUCCESS
        # ====================================================================
        logger.info(f"\n{'='*80}")
        logger.info(f"‚úÖ PIPELINE EXECUTION SUCCESSFUL")
        logger.info(f"{'='*80}")
        logger.info(f"Job ID: {job_id}")
        logger.info(f"Pipeline: {pipeline_name}")
        logger.info(f"Status: {result['status']}")
        logger.info(f"Time: {execution_time:.2f}s")
        logger.info("")

        return result

    except Exception as e:
        # ====================================================================
        # ERROR HANDLING
        # ====================================================================
        logger.error(f"\n{'='*80}")
        logger.error(f"‚ùå PIPELINE EXECUTION FAILED")
        logger.error(f"{'='*80}")
        logger.error(f"Job ID: {job_id}")
        logger.error(f"Pipeline: {pipeline_name}")
        logger.error(f"Error Type: {type(e).__name__}")
        logger.error(f"Error Message: {str(e)}", exc_info=True)

        # Prepare error result
        execution_time = (datetime.utcnow() - job_start_time).total_seconds()

        error_result = {
            "status": "failed",
            "pipeline_name": pipeline_name,
            "error_type": type(e).__name__,
            "error_message": str(e),
            "execution_time": execution_time,
            "timestamp": job_start_time.isoformat()
        }

        # Store error in database
        try:
            error_msg = f"{type(e).__name__}: {str(e)}"
            db_manager.update_job_error(job_id, error_msg)
            logger.info(f"‚úÖ Error logged to database")
        except Exception as log_error:
            logger.error(f"‚ùå Failed to log error: {log_error}")

        return error_result


@app.task(name='app.tasks.process_data', bind=True)
def process_data(self, dataset_id: str, processing_type: str, parameters: dict = None):
    """
    Process dataset with specified processing type

    Args:
        dataset_id (str): Unique dataset identifier
        processing_type (str): Type of processing to apply
        parameters (dict): Processing parameters

    Returns:
        dict: Processing result with status and metadata
    """
    start_time = datetime.utcnow()
    logger.info(f"{'='*80}")
    logger.info(f"üìä STARTING DATA PROCESSING")
    logger.info(f"{'='*80}")
    logger.info(f"Dataset ID: {dataset_id}")
    logger.info(f"Processing Type: {processing_type}")
    logger.info(f"Parameters: {parameters or {}}")

    try:
        # Simulate processing (replace with actual logic)
        import time
        time.sleep(2)

        execution_time = (datetime.utcnow() - start_time).total_seconds()

        result = {
            "status": "completed",
            "dataset_id": dataset_id,
            "processing_type": processing_type,
            "parameters": parameters or {},
            "execution_time": execution_time,
            "message": f"Successfully processed {dataset_id} with {processing_type}",
            "timestamp": start_time.isoformat()
        }

        logger.info(f"‚úÖ Data processing COMPLETED")
        logger.info(f"   - Status: {result['status']}")
        logger.info(f"   - Execution Time: {execution_time:.2f}s")

        # Update database if needed
        try:
            logger.info(f"Storing processing results...")
            # Add your database update logic here if needed
            logger.info(f"‚úÖ Results stored")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è  Could not store results: {e}")

        return result

    except Exception as e:
        logger.error(f"{'='*80}")
        logger.error(f"‚ùå DATA PROCESSING FAILED")
        logger.error(f"{'='*80}")
        logger.error(f"Error: {str(e)}", exc_info=True)

        execution_time = (datetime.utcnow() - start_time).total_seconds()

        error_result = {
            "status": "failed",
            "dataset_id": dataset_id,
            "processing_type": processing_type,
            "error_message": str(e),
            "execution_time": execution_time,
            "timestamp": start_time.isoformat()
        }

        return error_result


@app.task(name='app.tasks.analyze_data', bind=True)
def analyze_data(self, dataset_id: str, analysis_type: str, parameters: dict = None):
    """
    Analyze dataset with specified analysis type

    Args:
        dataset_id (str): Unique dataset identifier
        analysis_type (str): Type of analysis to perform
        parameters (dict): Analysis parameters

    Returns:
        dict: Analysis result with status and metadata
    """
    start_time = datetime.utcnow()
    logger.info(f"{'='*80}")
    logger.info(f"üìà STARTING DATA ANALYSIS")
    logger.info(f"{'='*80}")
    logger.info(f"Dataset ID: {dataset_id}")
    logger.info(f"Analysis Type: {analysis_type}")
    logger.info(f"Parameters: {parameters or {}}")

    try:
        # Simulate analysis (replace with actual logic)
        import time
        time.sleep(2)

        execution_time = (datetime.utcnow() - start_time).total_seconds()

        result = {
            "status": "completed",
            "dataset_id": dataset_id,
            "analysis_type": analysis_type,
            "parameters": parameters or {},
            "execution_time": execution_time,
            "message": f"Successfully analyzed {dataset_id} with {analysis_type}",
            "timestamp": start_time.isoformat(),
            "analysis_results": {
                "rows_processed": 1000,
                "patterns_found": 5,
                "anomalies_detected": 2
            }
        }

        logger.info(f"‚úÖ Data analysis COMPLETED")
        logger.info(f"   - Status: {result['status']}")
        logger.info(f"   - Execution Time: {execution_time:.2f}s")
        logger.info(f"   - Rows Processed: {result['analysis_results']['rows_processed']}")

        # Update database if needed
        try:
            logger.info(f"Storing analysis results...")
            # Add your database update logic here if needed
            logger.info(f"‚úÖ Results stored")
        except Exception as e:
            logger.error(f"‚ö†Ô∏è  Could not store results: {e}")

        return result

    except Exception as e:
        logger.error(f"{'='*80}")
        logger.error(f"‚ùå DATA ANALYSIS FAILED")
        logger.error(f"{'='*80}")
        logger.error(f"Error: {str(e)}", exc_info=True)

        execution_time = (datetime.utcnow() - start_time).total_seconds()

        error_result = {
            "status": "failed",
            "dataset_id": dataset_id,
            "analysis_type": analysis_type,
            "error_message": str(e),
            "execution_time": execution_time,
            "timestamp": start_time.isoformat()
        }

        return error_result